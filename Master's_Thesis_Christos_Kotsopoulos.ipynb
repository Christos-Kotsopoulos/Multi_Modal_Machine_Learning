{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Before starting we manually split the images into to sub-images that conctained 1 leaf per image. As before every image could contain from 1 to 6 leaves. After we implemented a DNN algorithm U^2-net that removed the backround from each image and produced a .png file without the backround noise that would insert bias into our NNs."
      ],
      "metadata": {
        "id": "Ut0fNA9ntOLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Backround removal procedure ##\n",
        "\n",
        "images_dir = '/content/Test_images'\n",
        "if os.path.exists(images_dir):\n",
        "    shutil.rmtree(images_dir)\n",
        "    os.makedirs(images_dir)\n",
        "    print(f'Directory {images_dir} has been emptied and recreated.')\n",
        "else:\n",
        "    print(f'Directory {images_dir} does not exist. Creating it now.')\n",
        "    os.makedirs(images_dir)\n",
        "\n",
        "\n",
        "%cd /content\n",
        "!/usr/local/cuda/bin/nvcc --version\n",
        "!git clone https://github.com/shreyas-bk/U-2-Net\n",
        "%cd /content/U-2-Net\n",
        "print('making images directory')\n",
        "!mkdir images\n",
        "print('making results directory')\n",
        "!mkdir results\n",
        "print('importing...')\n",
        "from google.colab import files\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import numpy as np\n",
        "from PIL import Image as Img\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "print('Done!')\n",
        "\n",
        "%cd /content/Test_images\n",
        "uploaded = files.upload()\n",
        "%cd /content/U-2-Net\n",
        "\n",
        "\n",
        "!python -W ignore u2net_test.py\n",
        "\n",
        "image_dir = os.path.join(os.getcwd(), 'images')\n",
        "names = [name[:-4] for name in os.listdir(image_dir)]\n",
        "THRESHOLD = 0.9\n",
        "RESCALE = 255\n",
        "LAYER = 2\n",
        "COLOR = (0, 0, 0)\n",
        "THICKNESS = 4\n",
        "SAL_SHIFT = 100\n",
        "\n",
        "\n",
        "for name in names:\n",
        "\n",
        "\n",
        "  if name == '.ipynb_checkpo':\n",
        "    continue\n",
        "  output = load_img('/content/U-2-Net/results/'+name+'.png')\n",
        "  out_img = img_to_array(output)\n",
        "  out_img /= RESCALE\n",
        "\n",
        "  out_img[out_img > THRESHOLD] = 1\n",
        "  out_img[out_img <= THRESHOLD] = 0\n",
        "\n",
        "  shape = out_img.shape\n",
        "  a_layer_init = np.ones(shape = (shape[0],shape[1],1))\n",
        "  mul_layer = np.expand_dims(out_img[:,:,0],axis=2)\n",
        "  a_layer = mul_layer*a_layer_init\n",
        "  rgba_out = np.append(out_img,a_layer,axis=2)\n",
        "\n",
        "  input = load_img('/content/Test_images/'+name+'.jpg')\n",
        "  inp_img = img_to_array(input)\n",
        "  inp_img /= RESCALE\n",
        "\n",
        "  a_layer = np.ones(shape = (shape[0],shape[1],1))\n",
        "  rgba_inp = np.append(inp_img,a_layer,axis=2)\n",
        "\n",
        "  rem_back = (rgba_inp*rgba_out)\n",
        "  rem_back_scaled = rem_back*RESCALE\n",
        "\n",
        "  inp_img*=RESCALE\n",
        "  inp_img = np.append(inp_img,RESCALE*a_layer,axis=2)\n",
        "  inp_img = cv2.resize(inp_img,(int(shape[1]/3),int(shape[0]/3)))\n",
        "  rem_back = cv2.resize(rem_back_scaled,(int(shape[1]/3),int(shape[0]/3)))\n",
        "  result = np.concatenate((inp_img,rem_back),axis=1)\n",
        "  result_img = Img.fromarray(result.astype('uint8'), 'RGBA')\n",
        "  print('\\nINPUT                                    BACKGROUND REMOVED')\n",
        "  display(result_img)"
      ],
      "metadata": {
        "id": "vR6d-Ro1tAK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7Of7aSBgC_M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Reading the raw file and dropping the uneeded values ##\n",
        "\n",
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "b9zznxZwgPzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_names = list(SSR_data.columns)\n",
        "col_names_1 = col_names[::2]\n",
        "col_names_2 = col_names[1::2]"
      ],
      "metadata": {
        "id": "Rz33uc4ygQcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Generating list with unique alles in each genetic locus ##\n",
        "\n",
        "all_js = []\n",
        "Uniques = []\n",
        "for name1, name2 in zip(col_names_1, col_names_2):\n",
        "    y = SSR_data[name1]\n",
        "    i = list(y.unique())\n",
        "    z = SSR_data[name2]\n",
        "    j = list(z.unique())\n",
        "\n",
        "    for k in range(len(i)):\n",
        "      if (i[k] not in j) and (pd.isna(i[k])== False):\n",
        "        j.append(i[k])\n",
        "    print(j)\n",
        "    Uniques = Uniques + j\n",
        "    np.array(j)\n",
        "    all_js.append(j)"
      ],
      "metadata": {
        "id": "UxdyNnZVgZmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot the species distribution to our raw/non-augmented data ##\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "data = {'Petrea':36, 'Fraineto':15, 'Pubescens':7}\n",
        "species = list(data.keys())\n",
        "count = list(data.values())\n",
        "\n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "\n",
        "plt.bar(species, count, color ='maroon',\n",
        "        width = 0.4)\n",
        "\n",
        "plt.xlabel(\"Species\")\n",
        "plt.ylabel(\"No. of trees in each species\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rtVTDCOEgeFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating a file to see the unique alles combinations ##\n",
        "\n",
        "file = open(\"unique_combs.txt\", 'w')\n",
        "\n",
        "for name in concatenated_data_col_names_B:\n",
        "  y = final_binary[name]\n",
        "  i = list(y.unique())\n",
        "\n",
        "  print(name)\n",
        "  print(i)\n",
        "  print('\\n')\n",
        "  print(len(i))\n",
        "  print('\\n')\n",
        "\n",
        "    file.write(name)\n",
        "  file.write('\\n')\n",
        "  file.write('\\n')\n",
        "  file.write(str(i))\n",
        "  file.write('\\n')\n",
        "  file.write(str(len(i)))\n",
        "  file.write('\\n')\n",
        "  file.write('\\n')\n",
        "  file.write('\\n')\n",
        "\n",
        "\n",
        "file.close()"
      ],
      "metadata": {
        "id": "WbwLXu8fhTcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the one-hot encoding and doing it from every allele in each locus"
      ],
      "metadata": {
        "id": "Ja_du2Zwk458"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## One-hot encoding for the alleles in Pie215 locus ##\n",
        "\n",
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [218, 215, 194, 203, 209, 206, 212, 200, 197, 227, 188]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"Pie215\"])):\n",
        "    if SSR_data[\"Pie215\"][j] == l[i]:\n",
        "      SSR_data[\"Pie215\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"Pie215\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"Pie215.1\"])):\n",
        "    if SSR_data[\"Pie215.1\"][j] == l[i]:\n",
        "      SSR_data[\"Pie215.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"Pie215.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"Pie215-\" + str(l[i])\n",
        "  col_name_1 = \"Pie215.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"Pie215\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"Pie215.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "5XRrdS9-hjIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [79, 98, 84, 88, 85, 83, 81, 86, 77, 82]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"Pie223\"])):\n",
        "    if SSR_data[\"Pie223\"][j] == l[i]:\n",
        "      SSR_data[\"Pie223\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"Pie223\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"Pie223.1\"])):\n",
        "    if SSR_data[\"Pie223.1\"][j] == l[i]:\n",
        "      SSR_data[\"Pie223.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"Pie223.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"Pie223-\" + str(l[i])\n",
        "  col_name_1 = \"Pie223.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"Pie223\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"Pie223.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "ISdSv51MiBlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [217, 199, 211, 205, 220, 214, 233, 223, 208]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"Pie227\"])):\n",
        "    if SSR_data[\"Pie227\"][j] == l[i]:\n",
        "      SSR_data[\"Pie227\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"Pie227\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"Pie227.1\"])):\n",
        "    if SSR_data[\"Pie227.1\"][j] == l[i]:\n",
        "      SSR_data[\"Pie227.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"Pie227.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"Pie227-\" + str(l[i])\n",
        "  col_name_1 = \"Pie227.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"Pie227\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"Pie227.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "KztCnfEmiJct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [161, 174, 155, 171, 158, 164, 148]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"Pie239\"])):\n",
        "    if SSR_data[\"Pie239\"][j] == l[i]:\n",
        "      SSR_data[\"Pie239\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"Pie239\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"Pie239.1\"])):\n",
        "    if SSR_data[\"Pie239.1\"][j] == l[i]:\n",
        "      SSR_data[\"Pie239.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"Pie239.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"Pie239-\" + str(l[i])\n",
        "  col_name_1 = \"Pie239.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"Pie239\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"Pie239.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "kVaci9A1iTKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [171, 170, 179, 177, 160, 173, 166, 162, 164, 158, 149, 153, 140, 155, 147, 136, 144, 151]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"ZAG96\"])):\n",
        "    if SSR_data[\"ZAG96\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG96\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG96\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"ZAG96.1\"])):\n",
        "    if SSR_data[\"ZAG96.1\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG96.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG96.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"ZAG96-\" + str(l[i])\n",
        "  col_name_1 = \"ZAG96.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"ZAG96\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"ZAG96.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "pZo2zgKDib6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [96, 85, 87, 94, 106, 79, 81, 83]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"ZAG112\"])):\n",
        "    if SSR_data[\"ZAG112\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG112\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG112\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"ZAG112.1\"])):\n",
        "    if SSR_data[\"ZAG112.1\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG112.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG112.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"ZAG112-\" + str(l[i])\n",
        "  col_name_1 = \"ZAG112.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"ZAG112\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"ZAG112.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "o3wfHTajilK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [218.0, 226.0, 228.0, 232.0, 222.0, 235.0, 230.0, 224.0, nan, 220.0, 234.0, 196.0, 212.0, 216.0, 210.0, 200.0]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"MSQ13\"])):\n",
        "    if SSR_data[\"MSQ13\"][j] == l[i]:\n",
        "      SSR_data[\"MSQ13\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"MSQ13\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"MSQ13.1\"])):\n",
        "    if SSR_data[\"MSQ13.1\"][j] == l[i]:\n",
        "      SSR_data[\"MSQ13.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"MSQ13.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"MSQ13-\" + str(l[i])\n",
        "  col_name_1 = \"MSQ13.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"MSQ13\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"MSQ13.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "gdcBrYV8itbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [167.0, nan, 171.0, 193.0, 165.0, 173.0, 177.0, 175.0, 181.0, 169.0, 183.0, 187.0, 179.0, 161.0, 163.0, 157.0]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"ZAG20\"])):\n",
        "    if SSR_data[\"ZAG20\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG20\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG20\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"ZAG20.1\"])):\n",
        "    if SSR_data[\"ZAG20.1\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG20.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG20.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"ZAG20-\" + str(l[i])\n",
        "  col_name_1 = \"ZAG20.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"ZAG20\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"ZAG20.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "LaYnoblci9G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [175.0, 172.0, nan, 186.0, 179.0, 170.0, 168.0, 173.0, 174.0, 167.0, 176.0, 184.0, 171.0, 164.0, 177.0, 169.0, 159.0, 166.0, 161.0, 160.0, 163.0]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"ZAG15\"])):\n",
        "    if SSR_data[\"ZAG15\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG15\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG15\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"ZAG15.1\"])):\n",
        "    if SSR_data[\"ZAG15.1\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG15.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG15.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"ZAG15-\" + str(l[i])\n",
        "  col_name_1 = \"ZAG15.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"ZAG15\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"ZAG15.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "MMClb-RFjLHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [252.0, 279.0, 271.0, 256.0, 275.0, 254.0, 248.0, 272.0, 267.0, 257.0, 243.0, 268.0, 264.0, nan, 250.0, 270.0, 240.0]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"ZAG11\"])):\n",
        "    if SSR_data[\"ZAG11\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG11\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG11\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"ZAG11.1\"])):\n",
        "    if SSR_data[\"ZAG11.1\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG11.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG11.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"ZAG11-\" + str(l[i])\n",
        "  col_name_1 = \"ZAG11.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"ZAG11\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"ZAG11.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "_46y1lXUjYyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [197, 193, 189, 199, 203, 211, 195, 207, 185, 187, 201, 191, 204, 247, 183, 181]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"ZAG9\"])):\n",
        "    if SSR_data[\"ZAG9\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG9\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG9\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"ZAG9.1\"])):\n",
        "    if SSR_data[\"ZAG9.1\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG9.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG9.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"ZAG9-\" + str(l[i])\n",
        "  col_name_1 = \"ZAG9.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"ZAG9\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"ZAG9.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "HTTMGoiKjngj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [204.0, 222.0, 208.0, 234.0, 228.0, 224.0, 205.0, 218.0, 216.0, 220.0, 212.0, 210.0, 206.0, nan, 226.0, 202.0, 230.0]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"ZAG110\"])):\n",
        "    if SSR_data[\"ZAG110\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG110\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG110\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"ZAG110.1\"])):\n",
        "    if SSR_data[\"ZAG110.1\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG110.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG110.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"ZAG110-\" + str(l[i])\n",
        "  col_name_1 = \"ZAG110.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"ZAG110\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"ZAG110.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "_ro0Vwf5jxXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [115, 111, 110, 113]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"GOT021\"])):\n",
        "    if SSR_data[\"GOT021\"][j] == l[i]:\n",
        "      SSR_data[\"GOT021\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"GOT021\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"GOT021.1\"])):\n",
        "    if SSR_data[\"GOT021.1\"][j] == l[i]:\n",
        "      SSR_data[\"GOT021.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"GOT021.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"GOT021-\" + str(l[i])\n",
        "  col_name_1 = \"GOT021.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"GOT021\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"GOT021.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "pt27pzhFj7o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [156, 158, 154, 147, 143, 149, 145]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"GOT045\"])):\n",
        "    if SSR_data[\"GOT045\"][j] == l[i]:\n",
        "      SSR_data[\"GOT045\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"GOT045\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"GOT045.1\"])):\n",
        "    if SSR_data[\"GOT045.1\"][j] == l[i]:\n",
        "      SSR_data[\"GOT045.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"GOT045.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"GOT045-\" + str(l[i])\n",
        "  col_name_1 = \"GOT045.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"GOT045\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"GOT045.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "GQZPtrAUkDU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [204, 198, 207, 212, 201, 209, 193, 195, 178]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"GOT062\"])):\n",
        "    if SSR_data[\"GOT062\"][j] == l[i]:\n",
        "      SSR_data[\"GOT062\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"GOT062\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"GOT062.1\"])):\n",
        "    if SSR_data[\"GOT062.1\"][j] == l[i]:\n",
        "      SSR_data[\"GOT062.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"GOT062.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"GOT062-\" + str(l[i])\n",
        "  col_name_1 = \"GOT062.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"GOT062\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"GOT062.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "5464AmgRkRG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [218.0, nan, 225.0, 212.0, 215.0, 200.0, 216.0, 223.0, 209.0, 210.0, 202.0, 214.0, 226.0, 220.0, 222.0, 211.0, 208.0, 206.0]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"ZAG36\"])):\n",
        "    if SSR_data[\"ZAG36\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG36\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG36\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"ZAG36.1\"])):\n",
        "    if SSR_data[\"ZAG36.1\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG36.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG36.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"ZAG36-\" + str(l[i])\n",
        "  col_name_1 = \"ZAG36.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"ZAG36\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"ZAG36.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "SalEJMSokY8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "SSR_data = SSR_data.drop('Species', axis=1)\n",
        "SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "SSR_data = SSR_data.drop('Latitude', axis=1)\n",
        "\n",
        "l = [113.0, nan, 157.0, 135.0, 133.0, 131.0, 139.0, 123.0, 129.0, 127.0, 137.0, 115.0, 119.0, 125.0, 147.0, 121.0]\n",
        "\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(SSR_data[\"ZAG7\"])):\n",
        "    if SSR_data[\"ZAG7\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG7\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG7\"][j] = 0\n",
        "  for j in range(len(SSR_data[\"ZAG7.1\"])):\n",
        "    if SSR_data[\"ZAG7.1\"][j] == l[i]:\n",
        "      SSR_data[\"ZAG7.1\"][j] = 1\n",
        "    else:\n",
        "      SSR_data[\"ZAG7.1\"][j] = 0\n",
        "\n",
        "\n",
        "  col_name = \"ZAG7-\" + str(l[i])\n",
        "  col_name_1 = \"ZAG7.1-\" + str(l[i])\n",
        "  df.loc[:, col_name] = SSR_data[\"ZAG7\"]\n",
        "  df.loc[:, col_name_1] = SSR_data[\"ZAG7.1\"]\n",
        "\n",
        "\n",
        "  SSR_data = pd.read_excel('SSR_data.xlsx')\n",
        "  SSR_data = SSR_data.drop('Sample', axis=1)\n",
        "  SSR_data = SSR_data.drop('Species', axis=1)\n",
        "  SSR_data = SSR_data.drop('Longitude', axis=1)\n",
        "  SSR_data = SSR_data.drop('Latitude', axis=1)"
      ],
      "metadata": {
        "id": "zyYjenjokagH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After creating the one-hot encoded file, we augment in and then we upload the augmented file that we will work on it from now on"
      ],
      "metadata": {
        "id": "vm2onqFAk-06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fin_oh = pd.read_excel('/content/Augmented_fin_fin_ohe.xlsx')\n",
        "print(fin_oh.shape)\n",
        "print(fin_oh.size)"
      ],
      "metadata": {
        "id": "OH3F2c3Xk7TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Interger encoding the labels from petraea,frainetto,pubescens to 0,1,2 respectively to the augmented panda frame  ##\n",
        "\n",
        "fin_oh['Species'] = fin_oh['Species'].map({'petraea':0,'frainetto':1,'pubescens':2})\n",
        "species_labels = fin_oh['Species']"
      ],
      "metadata": {
        "id": "YIaQM_XclMeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading the keras libraries ##\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Input, Flatten, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "EH9MtlVul867"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Running the NN to the non-augmented file ##\n",
        "\n",
        "\n",
        "fin_oh = pd.read_excel('/content/fin_fin_ohe.xlsx')\n",
        "fin_oh['Species'] = fin_oh['Species'].map({'petraea':0,'frainetto':1,'pubescens':2})\n",
        "species_labels = fin_oh['Species']\n",
        "\n",
        "\n",
        "data = fin_oh\n",
        "X = data.drop('Species', axis=1).values\n",
        "y = data['Species'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=10, validation_split=0.3,callbacks=[early_stopping])\n",
        "\n",
        "print(history.history)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_labels)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "acc = history.history.get('accuracy', [])\n",
        "val_acc = history.history.get('val_accuracy', [])\n",
        "loss = history.history.get('loss', [])\n",
        "val_loss = history.history.get('val_loss', [])\n",
        "\n",
        "if len(acc) > 0 and len(val_acc) > 0:\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, label='Training Loss')\n",
        "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No history data available for plotting.\")\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "DIRt1a-6lj0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating the NN branch that will handle the augmented genetic data ##\n",
        "\n",
        "data = fin_oh\n",
        "\n",
        "X = data.drop('Species', axis=1).values\n",
        "y = data['Species'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "genetic_input = Input(shape=(X_train.shape[1],))\n",
        "genetic_output = Dense(12, activation='swish')(genetic_input)\n",
        "genetic_output = Dropout(0.5)(genetic_output)\n",
        "genetic_output = Dense(8, activation='swish')(genetic_output)\n",
        "genetic_output = Dense(3, activation='softmax')(genetic_output)\n",
        "\n",
        "genetic_model = Model(inputs=genetic_input, outputs=genetic_output)\n",
        "\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "genetic_model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = genetic_model.fit(X_train, y_train, epochs=20, batch_size=10, validation_split=0.3)\n",
        "\n",
        "y_pred = genetic_model.predict(X_test)\n",
        "\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_labels)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "qMQAKL5PnRkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = genetic_model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "IATwAcb8okKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Saving the model and the outputs ##\n",
        "\n",
        "genetic_model.save('genetic_model.h5')\n",
        "genetic_outputs = genetic_model.predict(X_test)\n",
        "np.save('genetic_outputs.npy', genetic_outputs)"
      ],
      "metadata": {
        "id": "hLPGG94YoyGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading the file containing the curated/backround removed pictures ##\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "_390pFoips7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Some functions for the image branch, to delete directories concerning the images (This is done for code reruns as collab does not overwrite directories) ##\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "folder_path = '/content/preprocessed_images'\n",
        "\n",
        "if os.path.exists(folder_path):\n",
        "    shutil.rmtree(folder_path)\n",
        "    print(f\"Successfully deleted the folder: {folder_path}\")\n",
        "else:\n",
        "    print(f\"The folder does not exist: {folder_path}\")\n",
        "    import shutil\n",
        "import os\n",
        "\n",
        "folder_path = '/content/Sorted'\n",
        "\n",
        "if os.path.exists(folder_path):\n",
        "    shutil.rmtree(folder_path)\n",
        "    print(f\"Successfully deleted the folder: {folder_path}\")\n",
        "else:\n",
        "    print(f\"The folder does not exist: {folder_path}\")\n",
        "\n",
        "folder_path = '/content/Final_Curated_Data'\n",
        "\n",
        "if os.path.exists(folder_path):\n",
        "    shutil.rmtree(folder_path)\n",
        "    print(f\"Successfully deleted the folder: {folder_path}\")\n",
        "else:\n",
        "    print(f\"The folder does not exist: {folder_path}\")"
      ],
      "metadata": {
        "id": "EheSk7WHo4pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Some functions to handle and sort the images folder, interger encode them e.t.c ##\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.utils import class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import cv2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, ResNet50\n",
        "\n",
        "\n",
        "petraea = list(range(1, 8)) + list(range(27,38)) + list(range(42,55)) + [9,23,25,58,59]\n",
        "print(petraea)\n",
        "frainetto = list(range(12,15)) + list(range(38,42)) + list(range(55,58)) + [8,19,20,22,24,26]\n",
        "print(frainetto)\n",
        "pubescens = list(range(15,19)) + [10,11,21]\n",
        "print(pubescens)\n",
        "\n",
        "class_names = ['Frainetto', 'Petraea', 'Pubescens']\n",
        "\n",
        "input_folder = '/content/Sorted/'\n",
        "train_folder = 'preprocessed_images/train'\n",
        "test_folder = 'preprocessed_images/test'\n",
        "\n",
        "os.makedirs(train_folder, exist_ok=True)\n",
        "os.makedirs(test_folder, exist_ok=True)\n",
        "\n",
        "source_dir = '/content/Final_Curated_Data/Final_Data_Resized'\n",
        "\n",
        "dest_dirs = {\n",
        "    'petraea': '/content/Sorted/Petraea',\n",
        "    'frainetto': '/content/Sorted/Frainetto',\n",
        "    'pubescens': '/content/Sorted/Pubescens'\n",
        "}\n",
        "\n",
        "for dir_path in dest_dirs.values():\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "def move_images(numbers, dest_dir):\n",
        "    for filename in os.listdir(source_dir):\n",
        "        if filename.endswith(\".png\"):\n",
        "            parts = filename.split('_')[0]\n",
        "            file_number = int(''.join(filter(str.isdigit, parts)))\n",
        "            if file_number in numbers:\n",
        "                source_path = os.path.join(source_dir, filename)\n",
        "                dest_path = os.path.join(dest_dir, filename)\n",
        "                shutil.move(source_path, dest_path)\n",
        "\n",
        "move_images(petraea, dest_dirs['petraea'])\n",
        "move_images(frainetto, dest_dirs['frainetto'])\n",
        "move_images(pubescens, dest_dirs['pubescens'])\n",
        "\n",
        "\n",
        "def load_npy_files(folder):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith('.npy'):\n",
        "            filepath = os.path.join(folder, filename)\n",
        "            image = np.load(filepath)\n",
        "            label_part = filename.split('_')[0]\n",
        "\n",
        "            if label_part == 'Frainetto':\n",
        "                label = 0\n",
        "            elif label_part == 'Petraea':\n",
        "                label = 1\n",
        "            elif label_part == 'Pubescens':\n",
        "                label = 2\n",
        "            images.append(image)\n",
        "            labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = img.resize((224, 224))\n",
        "    img = np.array(img).astype(np.float32)\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "image_filenames = []\n",
        "for class_folder in class_names:\n",
        "    class_path = os.path.join(input_folder, class_folder)\n",
        "    for filename in os.listdir(class_path):\n",
        "        if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "            image_filenames.append((class_folder, filename))\n",
        "\n",
        "train_filenames, test_filenames = train_test_split(image_filenames, test_size=0.2, random_state=42)\n",
        "\n",
        "for class_folder, filename in train_filenames:\n",
        "    image_path = os.path.join(input_folder, class_folder, filename)\n",
        "    preprocessed_image = preprocess_image(image_path)\n",
        "\n",
        "    output_path = os.path.join(train_folder, f\"{class_folder}_label_{class_names.index(class_folder)}_{os.path.splitext(filename)[0]}.npy\")\n",
        "    np.save(output_path, preprocessed_image)\n",
        "\n",
        "for class_folder, filename in test_filenames:\n",
        "    image_path = os.path.join(input_folder, class_folder, filename)\n",
        "    preprocessed_image = preprocess_image(image_path)\n",
        "\n",
        "    output_path = os.path.join(test_folder, f\"{class_folder}_label_{class_names.index(class_folder)}_{os.path.splitext(filename)[0]}.npy\")\n",
        "    np.save(output_path, preprocessed_image)\n",
        "\n",
        "train_images, train_labels = load_npy_files(train_folder)\n",
        "test_images, test_labels = load_npy_files(test_folder)\n",
        "\n",
        "\n",
        "print(f\"Number of training images: {train_images.shape[0]}\")\n",
        "print(f\"Number of test images: {test_images.shape[0]}\")\n",
        "\n",
        "print(f\"Shape of train labels: {train_labels.shape}\")\n",
        "print(f\"Shape of test labels: {test_labels.shape}\")"
      ],
      "metadata": {
        "id": "H3YPJfqzqEt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating the NN for the image branch ##\n",
        "\n",
        "num_classes = 3\n",
        "\n",
        "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "resnet_base.trainable = False\n",
        "\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "\n",
        "x = resnet_base(image_input)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "image_output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "image_model = Model(inputs=image_input, outputs=image_output)\n",
        "\n",
        "image_model.compile(optimizer=Adam(learning_rate=1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = image_model.fit(train_images, train_labels, batch_size=32, epochs=10, validation_data=(test_images, test_labels))\n",
        "\n",
        "test_loss, test_accuracy = image_model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "pred_labels = image_model.predict(test_images)\n",
        "pred_labels = np.argmax(pred_labels, axis=1)\n",
        "\n",
        "true_labels = test_labels\n",
        "\n",
        "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "report = classification_report(true_labels, pred_labels, target_names=class_names)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "ul7r8dAhqZkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Saving the image and results model ##\n",
        "\n",
        "image_model.save('image_model.h5')\n",
        "image_outputs = image_model.predict(test_images)\n",
        "np.save('image_outputs.npy', image_outputs)"
      ],
      "metadata": {
        "id": "E1oxtpfNqiX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating the combined model ##\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "merged = Concatenate()([image_output, genetic_output])\n",
        "\n",
        "x = Dense(12, activation='swish')(merged)\n",
        "x = Dense(8, activation='swish')(x)\n",
        "\n",
        "output = Dense(num_classes, activation='softmax')(x)  # or 'sigmoid' for binary classification\n",
        "\n",
        "combined_model = Model(inputs=[image_input, genetic_input], outputs=output)\n",
        "\n",
        "optimizer = Adam(learning_rate=1e-2)\n",
        "combined_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = combined_model.fit([train_images, X_train], y_train, batch_size=32, epochs=10, validation_split=0.2)\n",
        "\n",
        "test_loss, test_accuracy = combined_model.evaluate([test_images, X_test], y_test)\n",
        "print(f\"Combined Test accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "5Q8GpdMcqp3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Saving the combined model ##\n",
        "\n",
        "combined_model.save('combined_model.h5')"
      ],
      "metadata": {
        "id": "eGQgx58jqzUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the models over a 10 iteration loop to examine how they behave and generate the ribbon plots\n"
      ],
      "metadata": {
        "id": "7k88Ee8VuDLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## 10 iterations of the genetic NN ##\n",
        "\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "data = fin_oh\n",
        "\n",
        "X = data.drop('Species', axis=1).values\n",
        "y = data['Species'].values\n",
        "\n",
        "num_runs = 10\n",
        "epochs = 10\n",
        "\n",
        "val_accuracy_each_run = []\n",
        "test_accuracy_each_run = []\n",
        "val_loss_each_run = []\n",
        "test_loss_each_run = []\n",
        "\n",
        "for run in range(num_runs):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42 + run, stratify=y)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    genetic_input = Input(shape=(X_train.shape[1],))\n",
        "    genetic_output = Dense(12, activation='swish')(genetic_input)\n",
        "    genetic_output = Dropout(0.5)(genetic_output)\n",
        "    genetic_output = Dense(8, activation='swish')(genetic_output)\n",
        "    genetic_output = Dense(3, activation='softmax')(genetic_output)\n",
        "    genetic_model = Model(inputs=genetic_input, outputs=genetic_output)\n",
        "\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    genetic_model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    val_accuracy_current_run = []\n",
        "    val_loss_current_run = []\n",
        "    test_accuracy_current_run = []\n",
        "    test_loss_current_run = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        history = genetic_model.fit(\n",
        "            X_train, y_train,\n",
        "            epochs=1,\n",
        "            batch_size=10,\n",
        "            validation_split=0.3,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        val_accuracy_current_run.append(history.history['val_accuracy'][0])\n",
        "        val_loss_current_run.append(history.history['val_loss'][0])\n",
        "\n",
        "        test_loss, test_accuracy = genetic_model.evaluate(X_test, y_test, verbose=0)\n",
        "        test_accuracy_current_run.append(test_accuracy)\n",
        "        test_loss_current_run.append(test_loss)\n",
        "\n",
        "    val_accuracy_each_run.append(val_accuracy_current_run)\n",
        "    val_loss_each_run.append(val_loss_current_run)\n",
        "    test_accuracy_each_run.append(test_accuracy_current_run)\n",
        "    test_loss_each_run.append(test_loss_current_run)\n",
        "\n",
        "average_val_accuracy = np.mean(val_accuracy_each_run, axis=0)\n",
        "average_val_loss = np.mean(val_loss_each_run, axis=0)\n",
        "average_test_accuracy = np.mean(test_accuracy_each_run, axis=0)\n",
        "average_test_loss = np.mean(test_loss_each_run, axis=0)\n",
        "\n",
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sorted_val_accuracy = np.sort(val_accuracy_each_run, axis=0)\n",
        "for i in range(num_runs - 1):\n",
        "    alpha = 0.2 + (i / num_runs) * 0.8\n",
        "    plt.fill_between(epochs_range, sorted_val_accuracy[i], sorted_val_accuracy[i + 1], color='blue', alpha=alpha)\n",
        "plt.plot(epochs_range, average_val_accuracy, 'r-', label='Average Validation Accuracy', linewidth=3)\n",
        "plt.title(\"Validation Accuracy Ribbon Plot\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sorted_test_accuracy = np.sort(test_accuracy_each_run, axis=0)\n",
        "for i in range(num_runs - 1):\n",
        "    alpha = 0.2 + (i / num_runs) * 0.8\n",
        "    plt.fill_between(epochs_range, sorted_test_accuracy[i], sorted_test_accuracy[i + 1], color='green', alpha=alpha)\n",
        "plt.plot(epochs_range, average_test_accuracy, 'r-', label='Average Test Accuracy', linewidth=3)\n",
        "plt.title(\"Test Accuracy Ribbon Plot\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sorted_val_loss = np.sort(val_loss_each_run, axis=0)\n",
        "for i in range(num_runs - 1):\n",
        "    alpha = 0.2 + (i / num_runs) * 0.8\n",
        "    plt.fill_between(epochs_range, sorted_val_loss[i], sorted_val_loss[i + 1], color='blue', alpha=alpha)\n",
        "plt.plot(epochs_range, average_val_loss, 'r-', label='Average Validation Loss', linewidth=3)\n",
        "plt.title(\"Validation Loss Ribbon Plot\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "sorted_test_loss = np.sort(test_loss_each_run, axis=0)\n",
        "for i in range(num_runs - 1):\n",
        "    alpha = 0.2 + (i / num_runs) * 0.8\n",
        "    plt.fill_between(epochs_range, sorted_test_loss[i], sorted_test_loss[i + 1], color='green', alpha=alpha)\n",
        "plt.plot(epochs_range, average_test_loss, 'r-', label='Average Test Loss', linewidth=3)\n",
        "plt.title(\"Test Loss Ribbon Plot\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-yTibCWTubKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 10 iterations of the image NN ##\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.utils import class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import cv2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, ResNet50\n",
        "\n",
        "\n",
        "petraea = list(range(1, 8)) + list(range(27,38)) + list(range(42,55)) + [9,23,25,58,59]\n",
        "print(petraea)\n",
        "frainetto = list(range(12,15)) + list(range(38,42)) + list(range(55,58)) + [8,19,20,22,24,26]\n",
        "print(frainetto)\n",
        "pubescens = list(range(15,19)) + [10,11,21]\n",
        "print(pubescens)\n",
        "\n",
        "class_names = ['Frainetto', 'Petraea', 'Pubescens']\n",
        "\n",
        "input_folder = '/content/Sorted/'\n",
        "train_folder = 'preprocessed_images/train'\n",
        "test_folder = 'preprocessed_images/test'\n",
        "\n",
        "os.makedirs(train_folder, exist_ok=True)\n",
        "os.makedirs(test_folder, exist_ok=True)\n",
        "\n",
        "source_dir = '/content/Final_Data_Resized/Final_Data_Resized'\n",
        "\n",
        "dest_dirs = {\n",
        "    'petraea': '/content/Sorted/Petraea',\n",
        "    'frainetto': '/content/Sorted/Frainetto',\n",
        "    'pubescens': '/content/Sorted/Pubescens'\n",
        "}\n",
        "\n",
        "for dir_path in dest_dirs.values():\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "\n",
        "def move_images(numbers, dest_dir):\n",
        "    for filename in os.listdir(source_dir):\n",
        "        if filename.endswith(\".png\"):\n",
        "            parts = filename.split('_')[0]\n",
        "            file_number = int(''.join(filter(str.isdigit, parts)))\n",
        "            if file_number in numbers:\n",
        "                source_path = os.path.join(source_dir, filename)\n",
        "                dest_path = os.path.join(dest_dir, filename)\n",
        "                shutil.move(source_path, dest_path)\n",
        "\n",
        "move_images(petraea, dest_dirs['petraea'])\n",
        "move_images(frainetto, dest_dirs['frainetto'])\n",
        "move_images(pubescens, dest_dirs['pubescens'])\n",
        "\n",
        "\n",
        "def load_npy_files(folder):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.endswith('.npy'):\n",
        "            filepath = os.path.join(folder, filename)\n",
        "            image = np.load(filepath)\n",
        "            label_part = filename.split('_')[0]\n",
        "\n",
        "            if label_part == 'Frainetto':\n",
        "                label = 0\n",
        "            elif label_part == 'Petraea':\n",
        "                label = 1\n",
        "            elif label_part == 'Pubescens':\n",
        "                label = 2\n",
        "            images.append(image)\n",
        "            labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = img.resize((224, 224))\n",
        "    img = np.array(img).astype(np.float32)\n",
        "    img = preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "image_filenames = []\n",
        "for class_folder in class_names:\n",
        "    class_path = os.path.join(input_folder, class_folder)\n",
        "    for filename in os.listdir(class_path):\n",
        "        if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
        "            image_filenames.append((class_folder, filename))\n",
        "\n",
        "train_filenames, test_filenames = train_test_split(image_filenames, test_size=0.1, random_state=42)\n",
        "\n",
        "for class_folder, filename in train_filenames:\n",
        "    image_path = os.path.join(input_folder, class_folder, filename)\n",
        "    preprocessed_image = preprocess_image(image_path)\n",
        "\n",
        "    output_path = os.path.join(train_folder, f\"{class_folder}_label_{class_names.index(class_folder)}_{os.path.splitext(filename)[0]}.npy\")\n",
        "    np.save(output_path, preprocessed_image)\n",
        "\n",
        "for class_folder, filename in test_filenames:\n",
        "    image_path = os.path.join(input_folder, class_folder, filename)\n",
        "    preprocessed_image = preprocess_image(image_path)\n",
        "\n",
        "    output_path = os.path.join(test_folder, f\"{class_folder}_label_{class_names.index(class_folder)}_{os.path.splitext(filename)[0]}.npy\")\n",
        "    np.save(output_path, preprocessed_image)\n",
        "\n",
        "train_images, train_labels = load_npy_files(train_folder)\n",
        "test_images, test_labels = load_npy_files(test_folder)\n",
        "\n",
        "\n",
        "print(f\"Number of training images: {train_images.shape[0]}\")\n",
        "print(f\"Number of test images: {test_images.shape[0]}\")\n",
        "\n",
        "print(f\"Shape of train labels: {train_labels.shape}\")\n",
        "print(f\"Shape of test labels: {test_labels.shape}\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "num_classes = 3\n",
        "num_runs = 10\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "val_accuracy_each_run = []\n",
        "test_accuracy_each_run = []\n",
        "val_loss_each_run = []\n",
        "test_loss_each_run = []\n",
        "\n",
        "for run in range(num_runs):\n",
        "    resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    resnet_base.trainable = False\n",
        "\n",
        "    image_input = Input(shape=(224, 224, 3))\n",
        "    x = resnet_base(image_input)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    image_output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    image_model = Model(inputs=image_input, outputs=image_output)\n",
        "    image_model.compile(optimizer=Adam(learning_rate=1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    val_accuracy_current_run = []\n",
        "    val_loss_current_run = []\n",
        "    test_accuracy_current_run = []\n",
        "    test_loss_current_run = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        history = image_model.fit(train_images, train_labels, batch_size=batch_size, epochs=1,\n",
        "                                  validation_data=(test_images, test_labels), verbose=0)\n",
        "\n",
        "        val_accuracy_current_run.append(history.history['val_accuracy'][0])\n",
        "        val_loss_current_run.append(history.history['val_loss'][0])\n",
        "\n",
        "        test_loss, test_accuracy = image_model.evaluate(test_images, test_labels, verbose=0)\n",
        "        test_accuracy_current_run.append(test_accuracy)\n",
        "        test_loss_current_run.append(test_loss)\n",
        "\n",
        "    val_accuracy_each_run.append(val_accuracy_current_run)\n",
        "    val_loss_each_run.append(val_loss_current_run)\n",
        "    test_accuracy_each_run.append(test_accuracy_current_run)\n",
        "    test_loss_each_run.append(test_loss_current_run)\n",
        "\n",
        "average_val_accuracy = np.mean(val_accuracy_each_run, axis=0)\n",
        "average_val_loss = np.mean(val_loss_each_run, axis=0)\n",
        "average_test_accuracy = np.mean(test_accuracy_each_run, axis=0)\n",
        "average_test_loss = np.mean(test_loss_each_run, axis=0)\n",
        "\n",
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "for i in range(num_runs - 1):\n",
        "    alpha = 0.2 + (i / num_runs) * 0.8\n",
        "    plt.fill_between(epochs_range, np.sort(val_accuracy_each_run, axis=0)[i],\n",
        "                     np.sort(val_accuracy_each_run, axis=0)[i + 1], color='blue', alpha=alpha)\n",
        "plt.plot(epochs_range, average_val_accuracy, 'r-', label='Average Validation Accuracy', linewidth=3)\n",
        "plt.title(\"Validation Accuracy Ribbon Plot\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "for i in range(num_runs - 1):\n",
        "    alpha = 0.2 + (i / num_runs) * 0.8\n",
        "    plt.fill_between(epochs_range, np.sort(test_accuracy_each_run, axis=0)[i],\n",
        "                     np.sort(test_accuracy_each_run, axis=0)[i + 1], color='green', alpha=alpha)\n",
        "plt.plot(epochs_range, average_test_accuracy, 'r-', label='Average Test Accuracy', linewidth=3)\n",
        "plt.title(\"Test Accuracy Ribbon Plot\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "for i in range(num_runs - 1):\n",
        "    alpha = 0.2 + (i / num_runs) * 0.8\n",
        "    plt.fill_between(epochs_range, np.sort(val_loss_each_run, axis=0)[i],\n",
        "                     np.sort(val_loss_each_run, axis=0)[i + 1], color='blue', alpha=alpha)\n",
        "plt.plot(epochs_range, average_val_loss, 'r-', label='Average Validation Loss', linewidth=3)\n",
        "plt.title(\"Validation Loss Ribbon Plot\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "for i in range(num_runs - 1):\n",
        "    alpha = 0.2 + (i / num_runs) * 0.8\n",
        "    plt.fill_between(epochs_range, np.sort(test_loss_each_run, axis=0)[i],\n",
        "                     np.sort(test_loss_each_run, axis=0)[i + 1], color='green', alpha=alpha)\n",
        "plt.plot(epochs_range, average_test_loss, 'r-', label='Average Test Loss', linewidth=3)\n",
        "plt.title(\"Test Loss Ribbon Plot\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "S_ltjlYPNnYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 10 iterations of the combined NN ##\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "genetic_data = X\n",
        "image_data = np.concatenate([train_images, test_images], axis=0)\n",
        "labels = y\n",
        "\n",
        "num_runs = 10\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "val_accuracy_each_run = []\n",
        "test_accuracy_each_run = []\n",
        "val_loss_each_run = []\n",
        "test_loss_each_run = []\n",
        "\n",
        "for run in range(num_runs):\n",
        "    print(f\"Run {run + 1}/{num_runs}\")\n",
        "\n",
        "    X_train_gen, X_test_gen, y_train, y_test = train_test_split(\n",
        "        genetic_data, labels, test_size=0.1, random_state=42 + run, stratify=labels\n",
        "    )\n",
        "    X_train_img, X_test_img, y_train_img, y_test_img = train_test_split(\n",
        "        image_data, labels, test_size=0.1, random_state=42 + run, stratify=labels\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_gen = scaler.fit_transform(X_train_gen)\n",
        "    X_test_gen = scaler.transform(X_test_gen)\n",
        "\n",
        "    merged = Concatenate()([image_output, genetic_output])\n",
        "\n",
        "    x = Dense(12, activation='swish', kernel_regularizer=l2(0.01))(merged)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    combined_model = Model(inputs=[image_input, genetic_input], outputs=output)\n",
        "\n",
        "    optimizer = Adam(learning_rate=1e-3)\n",
        "    combined_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    val_accuracy_current_run = []\n",
        "    val_loss_current_run = []\n",
        "    test_accuracy_current_run = []\n",
        "    test_loss_current_run = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        history = combined_model.fit(\n",
        "            [X_train_img, X_train_gen], y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=1,\n",
        "            validation_split=0.3,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        val_accuracy_current_run.append(history.history['val_accuracy'][0])\n",
        "        val_loss_current_run.append(history.history['val_loss'][0])\n",
        "\n",
        "        test_loss, test_accuracy = combined_model.evaluate([X_test_img, X_test_gen], y_test, verbose=0)\n",
        "        test_accuracy_current_run.append(test_accuracy)\n",
        "        test_loss_current_run.append(test_loss)\n",
        "\n",
        "    val_accuracy_each_run.append(val_accuracy_current_run)\n",
        "    val_loss_each_run.append(val_loss_current_run)\n",
        "    test_accuracy_each_run.append(test_accuracy_current_run)\n",
        "    test_loss_each_run.append(test_loss_current_run)\n",
        "\n",
        "average_val_accuracy = np.mean(val_accuracy_each_run, axis=0)\n",
        "average_val_loss = np.mean(val_loss_each_run, axis=0)\n",
        "average_test_accuracy = np.mean(test_accuracy_each_run, axis=0)\n",
        "average_test_loss = np.mean(test_loss_each_run, axis=0)\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "for i in range(num_runs - 1):\n",
        "    alpha = 0.2 + (i / num_runs) * 0.8\n",
        "    plt.fill_between(epochs_range,\n",
        "                     np.sort(val_accuracy_each_run, axis=0)[i],\n",
        "                     np.sort(val_accuracy_each_run, axis=0)[i + 1],\n",
        "                     color='blue', alpha=alpha)\n",
        "plt.plot(epochs_range, average_val_accuracy, 'r-', label='Average Validation Accuracy', linewidth=3)\n",
        "plt.title(\"Validation Accuracy Ribbon Plot\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "for i in range(num_runs - 1):\n",
        "    alpha = 0.2 + (i / num_runs) * 0.8\n",
        "    plt.fill_between(epochs_range,\n",
        "                     np.sort(test_accuracy_each_run, axis=0)[i],\n",
        "                     np.sort(test_accuracy_each_run, axis=0)[i + 1],\n",
        "                     color='green', alpha=alpha)\n",
        "plt.plot(epochs_range, average_test_accuracy, 'r-', label='Average Test Accuracy', linewidth=3)\n",
        "plt.title(\"Test Accuracy Ribbon Plot\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "for i in range(num_runs - 1):\n",
        "    alpha = 0.2 + (i / num_runs) * 0.8\n",
        "    plt.fill_between(epochs_range,\n",
        "                     np.sort(val_loss_each_run, axis=0)[i],\n",
        "                     np.sort(val_loss_each_run, axis=0)[i + 1],\n",
        "                     color='blue', alpha=alpha)\n",
        "plt.plot(epochs_range, average_val_loss, 'r-', label='Average Validation Loss', linewidth=3)\n",
        "plt.title(\"Validation Loss Ribbon Plot\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "for i in range(num_runs - 1):\n",
        "    alpha = 0.2 + (i / num_runs) * 0.8\n",
        "    plt.fill_between(epochs_range,\n",
        "                     np.sort(test_loss_each_run, axis=0)[i],\n",
        "                     np.sort(test_loss_each_run, axis=0)[i + 1],\n",
        "                     color='green', alpha=alpha)\n",
        "plt.plot(epochs_range, average_test_loss, 'r-', label='Average Test Loss', linewidth=3)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cHXWZaSIOKvI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}